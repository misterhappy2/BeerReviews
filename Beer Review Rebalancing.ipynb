{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING -- REBALANCE THE DATA \n",
    "#### The baseline model is set, I'll try to improve the model.  I'll eliminate imbalance, using oversampling or undersampling.  <br>  Here's the baseline for three classes of beer styles:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         IPA       0.84      0.61      0.71      2664\n",
    "       Other       0.87      0.94      0.90     10883\n",
    "       Stout       0.68      0.55      0.61      1196\n",
    "   micro avg       0.85      0.85      0.85     14743\n",
    "   macro avg       0.80      0.70      0.74     14743\n",
    "weighted avg       0.85      0.85      0.84     14743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 80818\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brewery</th>\n",
       "      <th>style</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Rock Ale</td>\n",
       "      <td>Big Rock Brewery</td>\n",
       "      <td>Scottish Ale</td>\n",
       "      <td>3.90</td>\n",
       "      <td>smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flip Ale</td>\n",
       "      <td>Dogfish Head Craft Brewery</td>\n",
       "      <td>Old Ale</td>\n",
       "      <td>4.08</td>\n",
       "      <td>on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Almond Marzen Project - Beer Camp #26</td>\n",
       "      <td>Sierra Nevada Brewing Co.</td>\n",
       "      <td>Märzen / Oktoberfest</td>\n",
       "      <td>3.78</td>\n",
       "      <td>nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name                     brewery  \\\n",
       "0                               Big Rock Ale            Big Rock Brewery   \n",
       "1                                   Flip Ale  Dogfish Head Craft Brewery   \n",
       "2  The Almond Marzen Project - Beer Camp #26   Sierra Nevada Brewing Co.   \n",
       "\n",
       "                  style  rating  \\\n",
       "0          Scottish Ale    3.90   \n",
       "1               Old Ale    4.08   \n",
       "2  Märzen / Oktoberfest    3.78   \n",
       "\n",
       "                                                                                                                                                                                                                        review  \n",
       "0  smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...  \n",
       "1   on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...  \n",
       "2  nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT MODULES AND THE DATA SET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "df = pd.read_csv('beer.csv', header=0)\n",
    "df_copy = df  #save a copy of dataframe for reference. \n",
    "print('length',len(df))\n",
    "pd.set_option('max_colwidth', 220)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brentmarijensen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brewery</th>\n",
       "      <th>style</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Rock Ale</td>\n",
       "      <td>Big Rock Brewery</td>\n",
       "      <td>Scottish Ale</td>\n",
       "      <td>3.90</td>\n",
       "      <td>smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...</td>\n",
       "      <td>smell soft hop aroma signific malt scent one smell creami tast creami tradit irish flavor come tongu creami like cream ale close malt big butteri smooth hop uniqu sharp hop flavor easi satur well mix blend play compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flip Ale</td>\n",
       "      <td>Dogfish Head Craft Brewery</td>\n",
       "      <td>Old Ale</td>\n",
       "      <td>4.08</td>\n",
       "      <td>on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...</td>\n",
       "      <td>tap dfh rehoboth collab eatili cardamom red wine must golden orang head typic dfh yeast aroma spice mayb belgian influenc sweet spici somewhat fruiti much old ale characterist light still tasti cardamom add nice flav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Almond Marzen Project - Beer Camp #26</td>\n",
       "      <td>Sierra Nevada Brewing Co.</td>\n",
       "      <td>Märzen / Oktoberfest</td>\n",
       "      <td>3.78</td>\n",
       "      <td>nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...</td>\n",
       "      <td>nice auburn impress ton clariti solid inch white head aroma littl bit sweet nutti tast gave littl sweet stay away hop bitter rel light bodi noth almond came obviou kind fanci oktoberfest good realli chang anyth use a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name                     brewery  \\\n",
       "index                                                                          \n",
       "0                                   Big Rock Ale            Big Rock Brewery   \n",
       "1                                       Flip Ale  Dogfish Head Craft Brewery   \n",
       "2      The Almond Marzen Project - Beer Camp #26   Sierra Nevada Brewing Co.   \n",
       "\n",
       "                      style  rating  \\\n",
       "index                                 \n",
       "0              Scottish Ale    3.90   \n",
       "1                   Old Ale    4.08   \n",
       "2      Märzen / Oktoberfest    3.78   \n",
       "\n",
       "                                                                                                                                                                                                                            review  \\\n",
       "index                                                                                                                                                                                                                                \n",
       "0      smell  soft hop aroma with significant malt scents. this one smells very creamy. taste  and creamy it is. the traditional irish flavors come out at the tongue. this is creamy, not like a cream ale, but close. the m...   \n",
       "1       on tap at dfh rehoboth... collab with eatily... cardamom and red wine must. golden orange. .no head. typical dfh yeast aroma. ..some spice and maybe a belgian influence. sweet spicy and somewhat fruity.. not much ol...   \n",
       "2      nice auburn impressions, tons of clarity, solid inch of off white head.   aroma was a little bit sweet and nutty. taste gave a little more sweetness, stayed away from hops and bitterness, relatively light bodied.  no...   \n",
       "\n",
       "                                                                                                                                                                                                                      clean_review  \n",
       "index                                                                                                                                                                                                                               \n",
       "0      smell soft hop aroma signific malt scent one smell creami tast creami tradit irish flavor come tongu creami like cream ale close malt big butteri smooth hop uniqu sharp hop flavor easi satur well mix blend play compl...  \n",
       "1      tap dfh rehoboth collab eatili cardamom red wine must golden orang head typic dfh yeast aroma spice mayb belgian influenc sweet spici somewhat fruiti much old ale characterist light still tasti cardamom add nice flav...  \n",
       "2      nice auburn impress ton clariti solid inch white head aroma littl bit sweet nutti tast gave littl sweet stay away hop bitter rel light bodi noth almond came obviou kind fanci oktoberfest good realli chang anyth use a...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA PREP\n",
    "# drop all reviews with < 20 characters\n",
    "df = df[df['review'].map(len) > 20]\n",
    "# reset index for the shortened dataframe\n",
    "df['index'] = np.arange(len(df))\n",
    "df = df.set_index('index')\n",
    "\n",
    "# Change review to a string of words.  remove non-letters, make lower case, split into words.  \n",
    "# Remove stopwords (common words.)  Join back together into a long string of words. \n",
    "def review_to_words(review):\n",
    "    letters_only = re.sub('[^a-zA-Z]',' ', review)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words('english'))  \n",
    "    good_words = [w for w in words if not w in stops]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in good_words]\n",
    "    return(' '.join(stemmed))\n",
    "\n",
    "# clean the reviews\n",
    "df['clean_review'] = df['review'].apply(review_to_words)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 classes: Stout, IPA, and other\n",
    "\n",
    "three_styles = df \n",
    "ipa_list = ['American IPA','English India Pale Ale (IPA)','American Double / Imperial IPA',\n",
    "           'Belgian IPA',]\n",
    "three_styles['style'].replace(ipa_list, 'IPA', inplace=True)\n",
    "stout_list = ['American Stout','English Stout','Milk / Sweet Stout','Oatmeal Stout',\n",
    "             'Imperial Stout','American Double / Imperial Stout', ]\n",
    "three_styles['style'].replace(stout_list, 'Stout', inplace=True)\n",
    "other_list = ['Altbier', 'American Adjunct Lager', 'American Amber / Red Ale',\n",
    "       'American Amber / Red Lager', 'American Barleywine', 'American Black Ale', \n",
    "       'American Blonde Ale', 'American Brown Ale', 'American Double / Imperial Pilsner', \n",
    "        'American Pale Ale (APA)', 'American Pale Lager', 'American Pale Wheat Ale', \n",
    "       'American Porter', 'American Stout', 'American Strong Ale', 'American Wild Ale',\n",
    "       'Baltic Porter', 'Belgian Dark Ale', 'Belgian Pale Ale', 'Belgian Strong Dark Ale', \n",
    "       'Belgian Strong Pale Ale', 'Berliner Weissbier', 'Bière de Garde', 'Bock', \n",
    "       'California Common / Steam Beer', 'Chile Beer', 'Cream Ale', 'Czech Pilsener', \n",
    "       'Doppelbock', 'Dortmunder / Export Lager', 'Dubbel', 'Dunkelweizen', \n",
    "       'English Barleywine', 'English Bitter', 'English Brown Ale', 'English Pale Ale', \n",
    "       'English Dark Mild Ale',  'English Porter', 'English Strong Ale', 'Euro Dark Lager', \n",
    "       'Euro Pale Lager', 'Extra Special / Strong Bitter (ESB)', 'Flanders Oud Bruin', \n",
    "       'Flanders Red Ale', 'Foreign / Export Stout', 'Fruit / Vegetable Beer', \n",
    "       'German Pilsener', 'Gose', 'Hefeweizen', 'Herbed / Spiced Beer', 'Irish Dry Stout', \n",
    "       'Irish Red Ale', 'Kellerbier / Zwickelbier', 'Kölsch', 'Lambic - Fruit', \n",
    "       'Light Lager', 'Maibock / Helles Bock', 'Milk / Sweet Stout', 'Munich Dunkel Lager',  \n",
    "       'Munich Helles Lager', 'Märzen / Oktoberfest', 'Old Ale', 'Pumpkin Ale', \n",
    "       'Quadrupel (Quad)', 'Rauchbier', 'Russian Imperial Stout', 'Rye Beer',\n",
    "        'Saison / Farmhouse Ale', 'Schwarzbier', 'Scotch Ale / Wee Heavy', 'Scottish Ale', \n",
    "       'Smoked Beer', 'Tripel', 'Vienna Lager', 'Weizenbock', 'Wheatwine', \n",
    "       'Winter Warmer', 'Witbier','American Dark Wheat Ale', 'American Malt Liquor',       \n",
    "       'Bière de Champagne / Bière Brut', 'Black & Tan', 'Braggot', 'Eisbock',\n",
    "       'English Pale Mild Ale', 'Euro Strong Lager', 'Faro', 'Gueuze', 'Happoshu', \n",
    "       'Japanese Rice Lager', 'Kristalweizen', 'Kvass', 'Lambic - Unblended', \n",
    "       'Low Alcohol Beer', 'Roggenbier', 'Sahti', 'Scottish Gruit / Ancient Herbed Ale',\n",
    "       'American Lager','Barleywine','Bitter', 'Brown Ale', 'Farm Ale', 'Lager',\n",
    "       'Pale Ale', 'Porter','Wheat']\n",
    "three_styles['style'].replace(other_list, 'Other', inplace=True)\n",
    "styles = three_styles.groupby(['style']).size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style\n",
      "IPA       8945\n",
      "Other    36300\n",
      "Stout     3896\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels = three_styles.groupby(['style']).size() \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle the clean data:\n",
    "import pickle\n",
    "filename = '3styles'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(three_styles,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieve the pickled data:\n",
    "import pickle\n",
    "filename = '3styles'\n",
    "infile = open(filename,'rb')\n",
    "three_styles = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VECTORIZE THE REVIEWS  \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = three_styles['clean_review'].values\n",
    "y = three_styles['style'].values\n",
    "\n",
    "# vectorize the train data, fit and transform into feature vectors\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X_counts = vectorizer.fit_transform(X)\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_counts)\n",
    "scaler = Normalizer()\n",
    "X_scaled = scaler.fit_transform(X_train_tfidf)\n",
    "\n",
    "# split into train and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES to predict style\n",
    "#### Use classification report to see all scoring.  <br>  Compare train data and test data to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.91      0.75      0.82      6281\n",
      "       Other       0.92      0.96      0.94     25417\n",
      "       Stout       0.81      0.77      0.79      2700\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     34398\n",
      "   macro avg       0.88      0.83      0.85     34398\n",
      "weighted avg       0.91      0.91      0.91     34398\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.84      0.60      0.70      2664\n",
      "       Other       0.86      0.94      0.90     10883\n",
      "       Stout       0.68      0.55      0.61      1196\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     14743\n",
      "   macro avg       0.80      0.70      0.74     14743\n",
      "weighted avg       0.85      0.85      0.84     14743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES PREDICTOR  BASELINE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve\n",
    "\n",
    "clf = MultinomialNB(alpha = 0.01)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "predicted_train = clf.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train, predicted_train))\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stout performs much better in the training set than the test set.  Thus, Stout is overfitted, perhaps because of the low support number.  I'll address the imbalance with sampling methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning for multnomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01} 0.847781847782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'alpha': [1,0.1,0.01,0.001,0.0001]}\n",
    "clf = MultinomialNB()\n",
    "clf_cv = GridSearchCV(clf, param_grid, cv = 5)\n",
    "clf_cv.fit(X_train, y_train)\n",
    "print(clf_cv.best_params_,clf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to grid search, alpha=0.01 is the best setting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RE-SAMPLING DATA.  \n",
    "#### Use SMOTE to oversample the small classes.  This creates more data to bolster the small classes, making all classes balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# oversample using SMOTE to balance the classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X_scaled, y)\n",
    "# split into train and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report on the TRAIN data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.92      0.93      0.92     25285\n",
      "       Other       0.91      0.82      0.87     25476\n",
      "       Stout       0.90      0.97      0.94     25469\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     76230\n",
      "   macro avg       0.91      0.91      0.91     76230\n",
      "weighted avg       0.91      0.91      0.91     76230\n",
      "\n",
      "classification report on the TEST data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.90      0.92      0.91     11015\n",
      "       Other       0.89      0.78      0.83     10824\n",
      "       Stout       0.89      0.97      0.93     10831\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     32670\n",
      "   macro avg       0.89      0.89      0.89     32670\n",
      "weighted avg       0.89      0.89      0.89     32670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES PREDICTOR \n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve\n",
    "clf = MultinomialNB(alpha = 0.01)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "predicted_train = clf.predict(X_train)\n",
    "\n",
    "print(\"classification report on the TRAIN data\")\n",
    "print(classification_report(y_train, predicted_train))\n",
    "print(\"classification report on the TEST data\")\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this boosted the scores a lot, and removed the overfitting problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNDERSAMPLING  <br>  try with random undersampling, then try NearMiss undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# undersample using random to balance the classes\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_scaled, y)\n",
    "# split into train and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report on the TRAIN data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.95      0.96      0.95      2703\n",
      "       Other       0.97      0.87      0.92      2731\n",
      "       Stout       0.91      0.99      0.95      2747\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      8181\n",
      "   macro avg       0.94      0.94      0.94      8181\n",
      "weighted avg       0.94      0.94      0.94      8181\n",
      "\n",
      "classification report on the TEST data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.86      0.85      0.85      1193\n",
      "       Other       0.79      0.71      0.74      1165\n",
      "       Stout       0.84      0.93      0.88      1149\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      3507\n",
      "   macro avg       0.83      0.83      0.83      3507\n",
      "weighted avg       0.83      0.83      0.83      3507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES PREDICTOR \n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve\n",
    "clf = MultinomialNB(alpha = 0.01)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "predicted_train = clf.predict(X_train)\n",
    "\n",
    "print(\"classification report on the TRAIN data\")\n",
    "print(classification_report(y_train, predicted_train))\n",
    "print(\"classification report on the TEST data\")\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# undersample using NearMiss to balance the classes\n",
    "from imblearn.under_sampling import NearMiss\n",
    "nm1 = NearMiss(version=1)\n",
    "X_resampled_nm1, y_resampled = nm1.fit_resample(X_scaled, y)\n",
    "# split into train and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report on the TRAIN data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.95      0.96      0.95      2703\n",
      "       Other       0.97      0.87      0.92      2731\n",
      "       Stout       0.91      0.99      0.95      2747\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      8181\n",
      "   macro avg       0.94      0.94      0.94      8181\n",
      "weighted avg       0.94      0.94      0.94      8181\n",
      "\n",
      "classification report on the TEST data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.86      0.85      0.85      1193\n",
      "       Other       0.79      0.71      0.74      1165\n",
      "       Stout       0.84      0.93      0.88      1149\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      3507\n",
      "   macro avg       0.83      0.83      0.83      3507\n",
      "weighted avg       0.83      0.83      0.83      3507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES PREDICTOR \n",
    "%%time\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve\n",
    "clf = MultinomialNB(alpha = 0.01)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "predicted_train = clf.predict(X_train)\n",
    "\n",
    "print(\"classification report on the TRAIN data\")\n",
    "print(classification_report(y_train, predicted_train))\n",
    "print(\"classification report on the TEST data\")\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using SMOTE to oversample the small classes worked the best of the sampling methods I tried.  (There are other possible sampling methods, but I'm satisfied with this.)  \n",
    "#### However, I see that I sampled before train test split.  This altered the test data.  I'll do it again, sampling after the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 4.65 s, total: 1min 17s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# oversample using SMOTE to balance the classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# VECTORIZE THE REVIEWS  \n",
    "X = three_styles['clean_review'].values\n",
    "y = three_styles['style'].values\n",
    "\n",
    "# vectorize the train data, fit and transform into feature vectors\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X_counts = vectorizer.fit_transform(X)\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_counts)\n",
    "scaler = Normalizer()\n",
    "X_scaled = scaler.fit_transform(X_train_tfidf)\n",
    "\n",
    "# split into train and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y, test_size=0.3, random_state=22)\n",
    "# split into train and test data\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report on the TRAIN data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.91      0.95      0.93     25417\n",
      "       Other       0.94      0.83      0.88     25417\n",
      "       Stout       0.91      0.97      0.94     25417\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     76251\n",
      "   macro avg       0.92      0.92      0.92     76251\n",
      "weighted avg       0.92      0.92      0.92     76251\n",
      "\n",
      "classification report on the TEST data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IPA       0.65      0.85      0.74      2664\n",
      "       Other       0.94      0.79      0.86     10883\n",
      "       Stout       0.47      0.85      0.60      1196\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     14743\n",
      "   macro avg       0.69      0.83      0.73     14743\n",
      "weighted avg       0.85      0.80      0.81     14743\n",
      "\n",
      "CPU times: user 1.92 s, sys: 27.8 ms, total: 1.95 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NAIVE BAYES PREDICTOR \n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve\n",
    "clf = MultinomialNB(alpha = 0.01)\n",
    "clf.fit(X_resampled, y_resampled)\n",
    "predicted = clf.predict(X_test)\n",
    "predicted_train = clf.predict(X_resampled)\n",
    "\n",
    "print(\"classification report on the TRAIN data\")\n",
    "print(classification_report(y_resampled, predicted_train))\n",
    "print(\"classification report on the TEST data\")\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " BASELINE: \n",
    "    IPA       0.84      0.61      0.71      2664\n",
    "   Other       0.87      0.94      0.90     10883\n",
    "   Stout       0.68      0.55      0.61      1196\n",
    "    \n",
    "    weighted avg 0.85 0.85 0.84 14743"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
